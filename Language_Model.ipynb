{"cells":[{"cell_type":"markdown","source":["Let's first import all the necessary libraries and mount the drive to the notebook inorder to read text files and save the trained LSTM model."],"metadata":{"id":"dfBdHjbuFXad"},"id":"dfBdHjbuFXad"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ef73685-c364-48cd-8a34-81208228f10d"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import spacy\n","from collections import Counter\n","from IPython.display import clear_output"],"id":"0ef73685-c364-48cd-8a34-81208228f10d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3539,"status":"ok","timestamp":1667291651239,"user":{"displayName":"Shashank Banakar","userId":"04113662217198851854"},"user_tz":-330},"id":"7kGw76TjnhXL","outputId":"1629848c-29b3-42e1-f393-89e5776d4a97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"7kGw76TjnhXL"},{"cell_type":"markdown","source":["text is a variable that stores the contents of a txt file, which is later sliced inorder to use only a million words from the huge corpus."],"metadata":{"id":"JnaJ3n6HFrj5"},"id":"JnaJ3n6HFrj5"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2098,"status":"ok","timestamp":1667291653331,"user":{"displayName":"Shashank Banakar","userId":"04113662217198851854"},"user_tz":-330},"id":"e6d60033-2516-487e-8d47-19efe53854bf","outputId":"18f65ef4-1023-4161-c9c5-9e2b63222ba7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Language_Modeling\n","1000000\n"]}],"source":["%cd /content/drive/MyDrive/Language_Modeling/\n","\n","with open('feedback_prize.txt', 'r') as f:\n","    text = f.read()\n","\n","text = text[:1000000]\n","print(len(text))"],"id":"e6d60033-2516-487e-8d47-19efe53854bf"},{"cell_type":"code","source":["text[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"8RsJeWlvIXuj","executionInfo":{"status":"ok","timestamp":1667291658110,"user_tz":-330,"elapsed":660,"user":{"displayName":"Shashank Banakar","userId":"04113662217198851854"}},"outputId":"419009d4-7fb9-4ff3-d24f-55fc4c5539f6"},"id":"8RsJeWlvIXuj","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I think we should be able to play in a sport if we have a grade C. I think i would be not fear for s'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["One of the important steps in Langauge Modeling is to create word and integer mappings. After creating such mappings to convert words to integers and vice versa, it becomes necessary to convert the corpus to a sequence of integer values since Neural Networks only work with numbers."],"metadata":{"id":"nO9A28YCGX4R"},"id":"nO9A28YCGX4R"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0abf5859-474a-4994-9470-e6835f930405"},"outputs":[],"source":["!python -m spacy download en_core_web_md\n","# !pip install python -m spacy download en_core_web_lg\n","nlp = spacy.load('en_core_web_md')\n","clear_output()\n","\n","nlp.max_length = len(text)\n","\n","word_tokens_list = [word.text for word in nlp(text)]"],"id":"0abf5859-474a-4994-9470-e6835f930405"},{"cell_type":"code","execution_count":null,"metadata":{"id":"726ceca8-9558-41c6-92e3-f0715abf6b7d"},"outputs":[],"source":["unique_word_counter = Counter(word_tokens_list)"],"id":"726ceca8-9558-41c6-92e3-f0715abf6b7d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"239911c4-b479-403c-97fd-19d9027aa9b3"},"outputs":[],"source":["sorted_unique_word_counter = sorted(unique_word_counter.items(), key = lambda x: x[1], reverse = True)\n","sorted_unique_keys = [i for i, j in sorted_unique_word_counter]"],"id":"239911c4-b479-403c-97fd-19d9027aa9b3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4295e565-4354-495c-b37f-47af41700a5c"},"outputs":[],"source":["word2int = {word: idx for idx, word in enumerate(sorted_unique_keys)}\n","int2word = {idx: word for idx, word in enumerate(sorted_unique_keys)}"],"id":"4295e565-4354-495c-b37f-47af41700a5c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7843f70f-072a-4783-bef6-fb6119ee1267"},"outputs":[],"source":["intarr = [word2int[i] for i in word_tokens_list]"],"id":"7843f70f-072a-4783-bef6-fb6119ee1267"},{"cell_type":"markdown","source":["Next, for faster computation and to exploit the computational efficiency provided by GPUs, get_batches is a function that takes care of splitting the corpus into batches of given batch_size and sequence_length. Along with a batch, the function also yields the expected output for the given batch."],"metadata":{"id":"bIFLPwUSG0GR"},"id":"bIFLPwUSG0GR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0faa6d5e-6a69-4cd9-be29-fd030ad84a27"},"outputs":[],"source":["def get_batches(arr: list([int]), batch_size: int, seq_length: int) -> tuple([list, list]):\n","  '''\n","  Function Description:\n","    The function takes in a list of integer values, representing words, and a couple of batch parameters to yields a batch of words and corresponding output words. \n","\n","  Parameters:\n","    arr: a list of strings converted to integer values using word2int mapping\n","    batch_size: number of lists to be returned in a batch\n","    seq_length: length of a sequence in a batch\n","  \n","  Returns:\n","    A tuple containing 2 lists; a list of input words from a batch and a list of the corresponding output.\n","  '''\n","\n","  arr = np.array(arr)\n","  number_of_full_batches = len(arr) // (batch_size * seq_length)\n","  new_arr_size = number_of_full_batches * batch_size * seq_length\n","  arr = arr[ : new_arr_size]\n","  arr = arr.reshape(batch_size, -1)\n","  for i in range(0, arr.shape[1], seq_length):\n","      x = arr[ : , i : i+seq_length]\n","      y = np.zeros_like(x)\n","      y[ : , : -1] = x[ : , 1 : ]\n","      try:\n","          y[ : , -1] = arr[ : , i+seq_length]\n","      except IndexError:\n","          y[ : , -1] = arr[ : , 0]\n","      yield x, y"],"id":"0faa6d5e-6a69-4cd9-be29-fd030ad84a27"},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667291796115,"user":{"displayName":"Shashank Banakar","userId":"04113662217198851854"},"user_tz":-330},"id":"e30d0120-6638-47e0-819f-36accfb14081","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"2b08d1de-cce0-4372-f613-ba0ceab6ef66"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"id":"e30d0120-6638-47e0-819f-36accfb14081"},{"cell_type":"markdown","source":["The Recurrent Neural Network used for this project of Language Modeling is LSTM which can learn long range influences from a large text. Using the word embeddings, a fully connected layer and a LSTM, the class wordRNN wraps the RNN which is to be trained and tested. The class also has methods to feed forward through the network and also to create a initial hidden layer in the LSTM."],"metadata":{"id":"94Yn7ZQsIcDX"},"id":"94Yn7ZQsIcDX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0141b780-b1e2-4efc-8921-d8d045e2289e"},"outputs":[],"source":["class wordRNN(nn.Module):\n","  '''\n","  A class to instantiate a Long Short Term Memory Neural Network.\n","  .forward() takes a list of words which are word2int mapped and a hidden state to output a list of predicted words and a new hidden state.\n","  .init_hidden() creates a initial hidden state of the given batch_size.\n","  '''\n","\n","  def __init__(self, tokens, embedding_dim = 50, n_hidden = 512, n_layers = 2, drop_prob = 0.5):\n","      super().__init__()\n","      self.drop_prob = drop_prob\n","      self.words = tokens\n","      self.embedding_dim = embedding_dim\n","      self.n_vocab = len(set(self.words))\n","      self.n_hidden = n_hidden\n","      self.n_layers = n_layers\n","      self.dropout = nn.Dropout()\n","      self.fc = nn.Linear(self.n_hidden, self.n_vocab)\n","      self.embedding = nn.Embedding(self.n_vocab, self.embedding_dim)\n","      self.lstm = nn.LSTM(self.embedding_dim, self.n_hidden, self.n_layers, dropout = self.drop_prob, batch_first = True)\n","      \n","  def forward(self, x, hidden):\n","      x = torch.from_numpy(x)\n","      x = x.to(device)\n","      embed = self.embedding(x)\n","      r_output, hidden = self.lstm(embed, hidden)\n","      out = self.dropout(r_output)\n","      out = r_output.contiguous().view(-1, self.n_hidden)\n","      out = self.fc(out)\n","      return out, hidden\n","  \n","  def init_hidden(self, batch_size):\n","      \n","      hidden = (torch.zeros(self.n_layers, batch_size, self.n_hidden).to(device),\n","                torch.zeros(self.n_layers, batch_size, self.n_hidden).to(device))\n","    \n","      return hidden "],"id":"0141b780-b1e2-4efc-8921-d8d045e2289e"},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1667291869675,"user":{"displayName":"Shashank Banakar","userId":"04113662217198851854"},"user_tz":-330},"id":"0a32cb5b-67ea-4efd-9592-4f4445a3c35e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6dc0db59-a0e7-4eb6-f6d5-fb4d95864c72"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["wordRNN(\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=512, out_features=9355, bias=True)\n","  (embedding): Embedding(9355, 50)\n","  (lstm): LSTM(50, 512, num_layers=2, batch_first=True, dropout=0.5)\n",")"]},"metadata":{},"execution_count":16}],"source":["model = wordRNN(intarr)\n","model"],"id":"0a32cb5b-67ea-4efd-9592-4f4445a3c35e"},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"a26Yqv9a9zag"},"id":"a26Yqv9a9zag","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The created model is now trained and it's performance (perplexity) is checked on predicting on text which the model hasn't seen before using the validation dataset. The model is saved when the the validation loss is at it's least."],"metadata":{"id":"tSct6rJoK5qf"},"id":"tSct6rJoK5qf"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9f3a280c-caec-4f03-84a7-b53991f4dcba","executionInfo":{"status":"error","timestamp":1666535346891,"user_tz":-330,"elapsed":122799,"user":{"displayName":"Shashank Banakar","userId":"04113662217198851854"}},"outputId":"7053d028-d8e5-47f2-b86d-abf812452289"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------epoch-1----------------------\n","Training loss: 9.147414         batch 1 / 336\n","Training loss: 5.892900         batch 201 / 336\n","Validation: Average Loss =  6.015    Average Perplexity =  409.697\n","Saving the model..\n","\n","\n","----------------------epoch-2----------------------\n","Training loss: 5.929358         batch 1 / 336\n","Training loss: 5.443862         batch 201 / 336\n","Validation: Average Loss =  5.644    Average Perplexity =  282.638\n","Saving the model..\n","\n","\n","----------------------epoch-3----------------------\n","Training loss: 5.475732         batch 1 / 336\n","Training loss: 5.043805         batch 201 / 336\n","Validation: Average Loss =  5.464    Average Perplexity =  235.923\n","Saving the model..\n","\n","\n","----------------------epoch-4----------------------\n","Training loss: 5.191070         batch 1 / 336\n","Training loss: 4.877835         batch 201 / 336\n","Validation: Average Loss =  5.360    Average Perplexity =  212.647\n","Saving the model..\n","\n","\n","----------------------epoch-5----------------------\n","Training loss: 4.961967         batch 1 / 336\n","Training loss: 4.720043         batch 201 / 336\n","Validation: Average Loss =  5.321    Average Perplexity =  204.496\n","Saving the model..\n","\n","\n","----------------------epoch-6----------------------\n","Training loss: 4.819915         batch 1 / 336\n","Training loss: 4.594908         batch 201 / 336\n","Validation: Average Loss =  5.271    Average Perplexity =  194.698\n","Saving the model..\n","\n","\n","----------------------epoch-7----------------------\n","Training loss: 4.638146         batch 1 / 336\n","Training loss: 4.469151         batch 201 / 336\n","Validation: Average Loss =  5.256    Average Perplexity =  191.619\n","Saving the model..\n","\n","\n","----------------------epoch-8----------------------\n","Training loss: 4.509976         batch 1 / 336\n","Training loss: 4.342855         batch 201 / 336\n","Validation: Average Loss =  5.238    Average Perplexity =  188.385\n","Saving the model..\n","\n","\n","----------------------epoch-9----------------------\n","Training loss: 4.357502         batch 1 / 336\n","Training loss: 4.228632         batch 201 / 336\n","Validation: Average Loss =  5.230    Average Perplexity =  186.720\n","Saving the model..\n","\n","\n","----------------------epoch-10----------------------\n","Training loss: 4.192477         batch 1 / 336\n","Training loss: 4.166895         batch 201 / 336\n","Validation: Average Loss =  5.223    Average Perplexity =  185.441\n","Saving the model..\n","\n","\n","----------------------epoch-11----------------------\n","Training loss: 4.110134         batch 1 / 336\n","Training loss: 4.038042         batch 201 / 336\n","Validation: Average Loss =  5.207    Average Perplexity =  182.609\n","Saving the model..\n","\n","\n","----------------------epoch-12----------------------\n","Training loss: 3.988101         batch 1 / 336\n","Training loss: 3.970394         batch 201 / 336\n","Validation: Average Loss =  5.216    Average Perplexity =  184.128\n","\n","\n","----------------------epoch-13----------------------\n","Training loss: 3.848752         batch 1 / 336\n","Training loss: 3.836016         batch 201 / 336\n","Validation: Average Loss =  5.237    Average Perplexity =  188.131\n","\n","\n","----------------------epoch-14----------------------\n","Training loss: 3.805769         batch 1 / 336\n","Training loss: 3.716034         batch 201 / 336\n","Validation: Average Loss =  5.257    Average Perplexity =  191.903\n","\n","\n","----------------------epoch-15----------------------\n","Training loss: 3.708433         batch 1 / 336\n","Training loss: 3.689719         batch 201 / 336\n","Validation: Average Loss =  5.267    Average Perplexity =  193.814\n","\n","\n","----------------------epoch-16----------------------\n","Training loss: 3.588767         batch 1 / 336\n","Training loss: 3.593494         batch 201 / 336\n","Validation: Average Loss =  5.279    Average Perplexity =  196.231\n","\n","\n","----------------------epoch-17----------------------\n","Training loss: 3.547700         batch 1 / 336\n","Training loss: 3.527929         batch 201 / 336\n","Validation: Average Loss =  5.291    Average Perplexity =  198.602\n","\n","\n","----------------------epoch-18----------------------\n","Training loss: 3.383471         batch 1 / 336\n","Training loss: 3.419931         batch 201 / 336\n","Validation: Average Loss =  5.332    Average Perplexity =  206.854\n","\n","\n","----------------------epoch-19----------------------\n","Training loss: 3.367821         batch 1 / 336\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-9566c078876e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_no\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# def train_loop(batches, model, optimizer, criterion):\n","batch_size = 10\n","seq_length = 50\n","learning_rate = 0.001\n","epochs = 250\n","optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","min_val_loss = np.inf\n","model = model.to(device)\n","clip = 5\n","    \n","validation_percent = 0.2\n","total_batches = len(list(get_batches(intarr, batch_size, seq_length)))\n","no_of_validation_batches = int(validation_percent * total_batches)\n","no_of_train_batches = total_batches - no_of_validation_batches\n","\n","hidden = model.init_hidden(batch_size)\n","    \n","for epoch in range(epochs):\n","    val_loss = 0\n","    perplexity = 0\n","    print(f'----------------------epoch-{epoch+1}----------------------')\n","    batches = get_batches(intarr, batch_size, seq_length)\n","    for batch_no, (x, y) in enumerate(batches):\n","        \n","        \n","        if(batch_no+1 <= no_of_train_batches):\n","            model.train()\n","            \n","            hidden = tuple(each.data.to(device) for each in hidden)\n","\n","            logits, hidden = model(x, hidden)\n","\n","            x = torch.tensor(x)\n","            y = torch.tensor(y).to(device)\n","\n","            y = y.view(batch_size * seq_length).long()\n","            loss = criterion(logits, y)\n","\n","            optimizer.zero_grad()\n","            # loss.backward(retain_graph=True)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","            if batch_no % 100 == 0:\n","                print(f'Training loss:{loss: 7f}         batch {batch_no+1} / {no_of_train_batches}')\n","                \n","        \n","        else:\n","            model.eval()\n","            # hidden = tuple(each.data for each in hidden)\n","            logits, hidden = model(x, hidden)\n","            x = torch.tensor(x)\n","            y = torch.tensor(y).to(device)\n","\n","            y = y.view(batch_size * seq_length).long()\n","            \n","            val_loss += criterion(logits, y)        \n","    \n","    avg_val_loss = val_loss / no_of_validation_batches\n","    perplexity = torch.exp(avg_val_loss)\n","    print(f'Validation: Average Loss = {avg_val_loss : .3f}    Average Perplexity = {perplexity: .3f}')\n","    \n","    if (avg_val_loss < min_val_loss):\n","        print('Saving the model..')\n","        min_val_loss = avg_val_loss\n","        torch.save(model, 'language_model.pth')\n","        \n","    print('\\n')"],"id":"9f3a280c-caec-4f03-84a7-b53991f4dcba"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oOyLP7z-sRTI"},"outputs":[],"source":["model = torch.load('Copy of language_model.pth')"],"id":"oOyLP7z-sRTI"},{"cell_type":"markdown","source":["Now that the model is trained, the model can be used to generate text. \n","\n","But before that we create a fucntion that can feed a word a word and hidden state to the model to return a predicted word a new hidden state."],"metadata":{"id":"ekMlK3bdLbfm"},"id":"ekMlK3bdLbfm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a96a6de1-b9dc-4e00-b4dc-648971828c20"},"outputs":[],"source":["def predict(char: str, hidden: tuple((torch.tensor, torch.tensor))) -> tuple((str, tuple((torch.tensor, torch.tensor)))):\n","  '''\n","  Function Description:\n","    Takes in a word and a hidden state to pass through the LSTM network and returns one of the top 5 predicted words and a new hidden state. \n","\n","  Parameters:\n","    char: an input word\n","    hidden: a hidden state\n","\n","  Returns:\n","    A new word (str) and a new hidden state.\n","  '''\n","\n","  softmax = nn.Softmax(dim = 1)\n","  x = np.array([[word2int[char]]])\n","\n","  next_word, next_hidden = model(x, hidden)\n","  next_word_probabilities = softmax(next_word)\n","\n","  p, top_ch = next_word_probabilities.topk(5)\n","  top_ch = top_ch.cpu().numpy().squeeze()\n","  p = p.detach().cpu().numpy().squeeze()\n","\n","  char = np.random.choice(top_ch)\n","  return int2word[char], hidden"],"id":"a96a6de1-b9dc-4e00-b4dc-648971828c20"},{"cell_type":"markdown","source":["To generate text using the model, the user simply needs to provide a few words and the trained RNN model reads the text, and remembers the influences and generates text based on the corpus on which it was trained."],"metadata":{"id":"D87t-i_rMn8W"},"id":"D87t-i_rMn8W"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5484e031-2b05-41ab-8f92-f9712ca7dad1"},"outputs":[],"source":["def sample(model: wordRNN, input_words: str, predict_words: int):\n","  '''\n","  Function Description:\n","    Using a sequence of input words, the function returns a string of predicted words.\n","\n","  Parameters:\n","    model: An object of class wordRNN; an LSTM network\n","    input_words: A sequence of words wrapped in a string\n","    predict_words: Number of words to output as predicted words\n","\n","  Returns:\n","    A string containing input_words and all the predicted words.\n","  '''\n","  model.eval()\n","  hidden = model.init_hidden(1)\n","  generated_text = []\n","\n","  for word in input_words.split():\n","    next_word, hidden = predict(word, hidden)\n","    generated_text.append(word) \n","  \n","  for i in range(predict_words):\n","      next_word, hidden = predict(generated_text[-1], hidden)\n","      generated_text.append(next_word)\n","\n","  return ' '.join(generated_text)"],"id":"5484e031-2b05-41ab-8f92-f9712ca7dad1"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1667287318521,"user":{"displayName":"Shashank Banakar","userId":"04817489823930814710"},"user_tz":-330},"id":"225bf93d-b52a-47e1-b32c-46a63077e311","outputId":"920d1f44-78bb-4e6a-e4ad-61cfe6fa2a19"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I think i would be not fear for student that have a good grade like not the most people get a lot not just have a lot , they think I feel the way you think the next work . have just get a phone and do not a better \" . . . \\n I know the next . ( classroom world think it is good work and just do you know they do not just get better . . \\n I do to do help . ( learning to have just just do you just do you just just get more more important and have to do you just just just get more than it help make one person is not one people do you think that you just not not one of school , and do you just do you just have more and do not just do n\\'t just just do n\\'t really one person just get more than you think it also just do not a good grades . have more than a more than it will not just just do you have more and the next school , school work that is good work , and just not a phone get more more than the car \" This would be able help . \\n The way the next school learning learning work . . \\n I have more than you just get the face it is one \" \" ( phones help make one people have more important and do not not not one people get the next work and have the way a lot the next . \\n It be not a good . have just get better and just have a good grades most students help you know , and do to be good time it also just not not just get more important one person you just have just have the most students , school , school . \\n the car people get better . to be good . . to get the way a phone . have to help the way that students just do you know , school and the most of students , they just just just have a better . to do n\\'t help that is one people do n\\'t help . . \\n It do n\\'t really way a lot the face that is one way you have just have to be more than a lot of one of the most of school and just have just get to have a good learning time to help make more than one of one of a better . \\n It is good learning way that we just do help . to have to do n\\'t really school . \\n I do you know you do you think I think I do help you have just not not the car is that you have more to be able a good time to be more and the face , they have a more and the car world do you do help that we have more more than you know you just get more than it be a more more and the next . to be able to have more important time it also do you just do not one person just have the car world do n\\'t just do not the face the next time the car is not one way the way to have just do help make a better and have more than the face that students to help you just just not a better and just not the car is not a lot , \" . ( school and have the way you just just get the face that not a lot not just do help that students to do you do n\\'t do help make a lot , the most students just just just get the way the next time you do help make the most way a more than you know you have the face that students help you know they know that students , school . have a more and have the way you know you know , and have more important and have to help that students to be not not the face are not one of school , and the face , they do n\\'t do to get more to be a phone to be good learning learning work . . . have just not a lot not the next classroom \" This would be not just get a phone get better ) and just not one \" \" ( school and just have more to be a good time you have the way you know you have a better work that is good grades . to do n\\'t just get better and have just do to do help . to help the way to help the next work . to do to do not not a more than one way a better and just get better . ( phone to get a good . to do to do to get more important to help that students help make to get the car people just get the car students to be good learning learning to get more important and just get better ) , and just get the car is not one people get a more to do n\\'t really school learning work . have just have more to be a more more than one people are a more to be not one of one of a better \" ( learning learning work and just have the car students help . to get to help make the car students help make one of the face the car students help that you just just have a good learning time that not one way the way the most students to be more than you know you have a better work and the car people are not a good time it also help that we just just have a more than the face , \" \" ( phone to do'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}],"source":["sample(model, ' I think i would be not fear for student that have a good grade like', 100)"],"id":"225bf93d-b52a-47e1-b32c-46a63077e311"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":5}